{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Binary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step consists in building the binary dataset defined as:\n",
    "\n",
    "We then introduce the binary data set. Let $X^{(b)} = \\{x^{(b)}_l | 1 ≤ l ≤ n\\}$ be a binary data set derived from the set of $r$ basic partitionings as follows:\n",
    "\n",
    "$$\n",
    "x^{(b)}_l = \\left( x^{(b)}_{l,1}, \\ldots, x^{(b)}_{l,i}, \\ldots, x^{(b)}_{l,r} \\right) \\\\\n",
    "x^{(b)}_{l,i} = \\left( x^{(b)}_{l,i1}, \\ldots, x^{(b)}_{l,ij}, \\ldots, x^{(b)}_{l,iKii} \\right)\\\\\n",
    "x^{(b)}_{l,ij} = \\begin{cases} 1, & \\text{if } L_i(x_l) = j \\\\ 0, & \\text{otherwise} \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (150, 4)\n",
      "y shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir(\"..\") if \"notebook\" in os.getcwd() else None\n",
    "from src.KMeans import KMeans\n",
    "import config\n",
    "\n",
    "# np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "X = pd.read_csv(os.path.join(config.DATA_FOLDER, 'iris.csv'))\n",
    "X = X.sample(frac=1).reset_index(drop=True)\n",
    "y = X.pop('target')\n",
    "X = (X - X.mean()) / X.std()\n",
    "X = X.values\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Basic  Partitionings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To generate basic partitionings (BPs), we used the kmeans with squared Euclidean distance for\n",
    "UCI data sets.\n",
    "- Number of clusters:\n",
    "  - [Default] Random Parameter Selection (RPS): We randomized the number of clusters within an interval for each basic clustering within $[K,\\sqrt{n}]$.\n",
    "  - Random Feature Selection (RFS): two features randomly for each BP, and set the number of clusters to K for kmeans.\n",
    "- For each data set, 100 BPs are typically generated for consensus clustering (namely r = 100), and the weights of these BPs are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering Progress: 100%|██████████| 100/100 [00:00<00:00, 485.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_b shape: (150, 709)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    one_hot = np.zeros((len(labels), num_classes))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "r = config.R\n",
    "n = len(X)\n",
    "classes = len(y.unique())\n",
    "\n",
    "max_k = int(n**0.5) + 1\n",
    "ls_partitions = []\n",
    "ls_partitions_labels = []\n",
    "total_k = 0\n",
    "for i in tqdm(range(r), desc=\"Clustering Progress\"):\n",
    "    k = np.random.randint(classes, max_k) # Closed form both sides\n",
    "    model = KMeans(k, n_init=10)\n",
    "    model.fit(X)\n",
    "    partition_i = model.labels_\n",
    "    ls_partitions_labels.append(partition_i)\n",
    "    ls_partitions.append(one_hot_encode(partition_i, k))\n",
    "    total_k += k\n",
    "\n",
    "X_b = np.hstack(ls_partitions)\n",
    "\n",
    "print(\"X_b shape:\", X_b.shape)\n",
    "\n",
    "assert X_b.shape == (n, total_k), \"Error in shape of X_b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(partition_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([9, 1, 7, 1, 1, 7, 8, 9, 9, 0, 3, 8, 1, 8, 4, 7, 6, 4, 9, 0, 6, 4,\n",
       "        6, 5, 3, 0, 9, 1, 5, 5, 6, 9, 9, 9, 6, 3, 3, 7, 1, 9, 9, 9, 3, 8,\n",
       "        5, 7, 8, 9, 6, 5, 7, 3, 3, 4, 6, 1, 6, 0, 3, 3, 8, 4, 8, 1, 8, 9,\n",
       "        5, 9, 6, 7, 3, 7, 4, 6, 8, 5, 9, 9, 6, 9, 9, 1, 3, 0, 8, 8, 4, 1,\n",
       "        3, 8, 5, 0, 9, 6, 1, 9, 5, 1, 3, 5, 5, 4, 8, 0, 9, 1, 5, 3, 9, 0,\n",
       "        1, 9, 9, 6, 9, 4, 3, 3, 6, 1, 7, 7, 3, 0, 8, 3, 6, 5, 8, 5, 9, 4,\n",
       "        1, 7, 1, 7, 0, 5, 7, 0, 6, 5, 9, 4, 1, 9, 5, 9, 6, 5], dtype=int64),\n",
       " array([1, 5, 5, 5, 3, 5, 1, 1, 1, 0, 3, 1, 5, 7, 2, 5, 4, 2, 1, 0, 4, 2,\n",
       "        4, 1, 3, 0, 4, 3, 1, 1, 4, 4, 1, 1, 4, 3, 3, 5, 3, 1, 1, 1, 3, 1,\n",
       "        4, 5, 7, 1, 4, 1, 5, 3, 3, 2, 4, 3, 4, 2, 3, 3, 7, 7, 1, 5, 1, 1,\n",
       "        2, 1, 4, 5, 3, 5, 2, 3, 7, 1, 1, 1, 4, 1, 4, 3, 3, 0, 7, 7, 2, 3,\n",
       "        3, 7, 1, 2, 1, 4, 5, 4, 1, 5, 3, 1, 1, 2, 2, 2, 1, 5, 1, 3, 4, 0,\n",
       "        5, 4, 1, 4, 1, 2, 3, 3, 4, 3, 5, 5, 3, 0, 7, 3, 4, 1, 1, 1, 4, 2,\n",
       "        5, 5, 5, 5, 0, 1, 5, 2, 4, 2, 1, 2, 5, 1, 1, 1, 4, 1], dtype=int64)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_partitions_labels[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the Binary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering tools. Three types of consensus clustering methods, namely the K-means-based algorithm (KCC), the graph partitioning algorithm (GP), and the hierarchical algorithm (HCC), were employed for the comparison purpose.\n",
    "**In our work, only KCC is used and compared to the paper's one**.\n",
    "\n",
    "- Define utility functions\n",
    "\n",
    "\n",
    "Note: You may need to adjust the alignment of the columns depending on your markdown renderer.\n",
    "\n",
    "I hope this helps! Let me know if you have any other questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare different methods for contingency table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.contingency import crosstab\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "pi = KMeans(classes, n_init=10)\n",
    "pi.fit(X_b)\n",
    "pi = pi.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# data_crosstab = pd.crosstab(pi, \n",
    "# \t\t\t\t\t\t\tls_partitions_labels[0], \n",
    "# \t\t\t\t\t\t\tmargins = False)\n",
    "\n",
    "# 2.58 ms ± 158 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# contingency_matrix(pi, ls_partitions_labels[0])\n",
    "# 79.1 µs ± 1.12 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# crosstab(pi, ls_partitions_labels[0]).count\n",
    "# 23 µs ± 318 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Information from Contingency Table**\n",
    "\n",
    "The information that we need to extract from the contingency table is:\n",
    "- $p_{kj}^{(i)}$ = the ratio of objects in consensus cluster k that belong to cluster j based on partition i\n",
    "- $p_k$ = the number of objects in consensus cluster k\n",
    "- $P_k^{i}$ = $p_{kj}^{(i)}/p_k$\n",
    "- $P^{(i)}$ = the vector with the ratios of objects in each cluster based on partition i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 18, 18, 11, 18, 17, 13, 15, 29]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate contingency matrix (n)\n",
    "pi = KMeans(classes)\n",
    "pi.fit(X_b)\n",
    "pi = pi.labels_\n",
    "\n",
    "cont_i = crosstab(pi, ls_partitions_labels[0]).count\n",
    "cont_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07333333, 0.12      , 0.12      , 0.07333333, 0.12      ,\n",
       "        0.11333333, 0.08666667, 0.1       , 0.19333333]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get p-contingency matrix\n",
    "p_cont_i = cont_i / cont_i.sum()\n",
    "p_cont_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the p_k for each cluster in PI\n",
    "p_k = p_cont_i.sum(axis=1)\n",
    "p_k = p_k.reshape(-1, 1)\n",
    "p_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07333333, 0.12      , 0.12      , 0.07333333, 0.12      ,\n",
       "        0.11333333, 0.08666667, 0.1       , 0.19333333]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the P_k^i for each cluster in the partition i\n",
    "P_k_i = p_cont_i / p_k\n",
    "P_k_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07333333, 0.12      , 0.12      , 0.07333333, 0.12      ,\n",
       "       0.11333333, 0.08666667, 0.1       , 0.19333333])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a constant and it's the ratio of points in each cluster in pi_i\n",
    "P_i = p_cont_i.sum(axis=0)\n",
    "P_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12168889]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the norm of each row of P_k^i\n",
    "norm_P_k_i = np.linalg.norm(P_k_i, axis=1, ord=2).reshape(-1, 1) ** 2\n",
    "norm_P_k_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12168888888888892"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_P_i = np.linalg.norm(P_i, ord=2) ** 2\n",
    "norm_P_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12168889]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_k * norm_P_k_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.163336342344337e-17"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility = (p_k * norm_P_k_i).sum() - norm_P_i\n",
    "utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
