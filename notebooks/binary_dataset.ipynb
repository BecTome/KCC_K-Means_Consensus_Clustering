{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Binary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step consists in building the binary dataset defined as:\n",
    "\n",
    "We then introduce the binary data set. Let $X^{(b)} = \\{x^{(b)}_l | 1 ≤ l ≤ n\\}$ be a binary data set derived from the set of $r$ basic partitionings as follows:\n",
    "\n",
    "$$\n",
    "x^{(b)}_l = \\left( x^{(b)}_{l,1}, \\ldots, x^{(b)}_{l,i}, \\ldots, x^{(b)}_{l,r} \\right) \\\\\n",
    "x^{(b)}_{l,i} = \\left( x^{(b)}_{l,i1}, \\ldots, x^{(b)}_{l,ij}, \\ldots, x^{(b)}_{l,iKii} \\right)\\\\\n",
    "x^{(b)}_{l,ij} = \\begin{cases} 1, & \\text{if } L_i(x_l) = j \\\\ 0, & \\text{otherwise} \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (150, 4)\n",
      "y shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "os.chdir(\"..\") if \"notebook\" in os.getcwd() else None\n",
    "import config\n",
    "from tqdm import tqdm\n",
    "# np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "X = pd.read_csv(os.path.join(config.DATA_FOLDER, 'iris.csv'))\n",
    "X = X.sample(frac=1).reset_index(drop=True)\n",
    "y = X.pop('target')\n",
    "X = (X - X.mean()) / X.std()\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Basic  Partitionings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To generate basic partitionings (BPs), we used the kmeans with squared Euclidean distance for\n",
    "UCI data sets.\n",
    "- Number of clusters:\n",
    "  - [Default] Random Parameter Selection (RPS): We randomized the number of clusters within an interval for each basic clustering within $[K,\\sqrt{n}]$.\n",
    "  - Random Feature Selection (RFS): two features randomly for each BP, and set the number of clusters to K for kmeans.\n",
    "- For each data set, 100 BPs are typically generated for consensus clustering (namely r = 100), and the weights of these BPs are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering Progress: 100%|██████████| 100/100 [00:05<00:00, 19.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_b shape: (150, 739)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    one_hot = np.zeros((len(labels), num_classes))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "r = config.R\n",
    "n = len(X)\n",
    "classes = len(y.unique())\n",
    "\n",
    "max_k = int(n**0.5) + 1\n",
    "ls_partitions = []\n",
    "ls_partitions_labels = []\n",
    "total_k = 0\n",
    "for i in tqdm(range(r), desc=\"Clustering Progress\"):\n",
    "    k = np.random.randint(classes, max_k) # Closed form both sides\n",
    "    partition_i = KMeans(n_clusters=k, n_init=10, init='k-means++').fit(X).labels_\n",
    "    ls_partitions_labels.append(partition_i)\n",
    "    ls_partitions.append(one_hot_encode(partition_i, k))\n",
    "    total_k += k\n",
    "\n",
    "X_b = np.hstack(ls_partitions)\n",
    "\n",
    "print(\"X_b shape:\", X_b.shape)\n",
    "\n",
    "assert X_b.shape == (n, total_k), \"Error in shape of X_b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 2, 1, 1, 4, 5, 1, 2, 3, 3, 5, 0, 4, 2, 0, 0, 1, 4, 2, 4, 2, 4,\n",
       "        1, 2, 2, 5, 1, 2, 4, 2, 5, 4, 4, 2, 0, 5, 0, 2, 6, 1, 0, 1, 6, 0,\n",
       "        1, 2, 1, 4, 4, 5, 3, 6, 0, 1, 0, 1, 1, 6, 4, 5, 2, 2, 5, 0, 2, 1,\n",
       "        3, 2, 1, 4, 3, 1, 2, 0, 5, 6, 2, 5, 1, 0, 1, 2, 6, 0, 3, 5, 4, 1,\n",
       "        5, 0, 4, 4, 0, 1, 2, 2, 3, 3, 5, 2, 0, 4, 1, 2, 5, 4, 0, 0, 4, 1,\n",
       "        1, 6, 2, 3, 2, 0, 2, 5, 4, 0, 5, 0, 0, 0, 2, 0, 0, 0, 5, 3, 4, 2,\n",
       "        2, 0, 2, 0, 5, 2, 3, 2, 2, 4, 2, 2, 2, 1, 3, 1, 0, 2]),\n",
       " array([1, 0, 2, 3, 0, 2, 2, 0, 1, 1, 2, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 0,\n",
       "        2, 0, 1, 2, 3, 1, 0, 0, 2, 0, 0, 0, 1, 2, 1, 0, 3, 3, 1, 3, 3, 1,\n",
       "        3, 0, 3, 0, 0, 2, 1, 3, 1, 3, 1, 2, 3, 3, 0, 2, 0, 0, 2, 1, 0, 3,\n",
       "        1, 0, 2, 0, 1, 3, 0, 1, 2, 3, 0, 2, 3, 1, 3, 1, 3, 1, 1, 2, 0, 3,\n",
       "        2, 1, 0, 0, 1, 2, 0, 0, 1, 1, 2, 1, 1, 0, 2, 0, 2, 0, 1, 1, 0, 3,\n",
       "        3, 3, 0, 1, 0, 1, 0, 2, 0, 1, 2, 1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1,\n",
       "        0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 3, 1, 3, 1, 0])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_partitions_labels[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the Binary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering tools. Three types of consensus clustering methods, namely the K-means-based algorithm (KCC), the graph partitioning algorithm (GP), and the hierarchical algorithm (HCC), were employed for the comparison purpose.\n",
    "**In our work, only KCC is used and compared to the paper's one**.\n",
    "\n",
    "- Define utility functions\n",
    "\n",
    "\n",
    "Note: You may need to adjust the alignment of the columns depending on your markdown renderer.\n",
    "\n",
    "I hope this helps! Let me know if you have any other questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare different methods for contingency table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.contingency import crosstab\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "# pi = KMeans(n_clusters=classes, n_init=10, init='random').fit(X_b).labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# data_crosstab = pd.crosstab(pi, \n",
    "# \t\t\t\t\t\t\tls_partitions_labels[0], \n",
    "# \t\t\t\t\t\t\tmargins = False)\n",
    "# 2.89 ms ± 104 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# contingency_matrix(pi, ls_partitions_labels[0])\n",
    "# 75.5 µs ± 1.32 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# crosstab(pi, ls_partitions_labels[0]).count\n",
    "# 22.7 µs ± 347 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Information from Contingency Table**\n",
    "\n",
    "The information that we need to extract from the contingency table is:\n",
    "- $p_{kj}^{(i)}$ = the ratio of objects in consensus cluster k that belong to cluster j based on partition i\n",
    "- $p_k$ = the number of objects in consensus cluster k\n",
    "- $P_k^{i}$ = $p_{kj}^{(i)}/p_k$\n",
    "- $P^(i) = the vector with the ratios of objects in each cluster based on partition i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, 35,  0, 21,  0,  0],\n",
       "       [30,  0,  2, 12,  0,  0,  0],\n",
       "       [ 0, 25,  0,  0,  0, 18,  7]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate contingency matrix (n)\n",
    "pi = KMeans(n_clusters=classes, n_init=10, init='k-means++').fit(X_b).labels_\n",
    "\n",
    "cont_i = crosstab(pi, ls_partitions_labels[0]).count\n",
    "cont_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.23333333, 0.        , 0.14      ,\n",
       "        0.        , 0.        ],\n",
       "       [0.2       , 0.        , 0.01333333, 0.08      , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.16666667, 0.        , 0.        , 0.        ,\n",
       "        0.12      , 0.04666667]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get p-contingency matrix\n",
    "p_cont_i = cont_i / cont_i.sum()\n",
    "p_cont_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37333333],\n",
       "       [0.29333333],\n",
       "       [0.33333333]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the p_k for each cluster in PI\n",
    "p_k = p_cont_i.sum(axis=1)\n",
    "p_k = p_k.reshape(-1, 1)\n",
    "p_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.625     , 0.        , 0.375     ,\n",
       "        0.        , 0.        ],\n",
       "       [0.68181818, 0.        , 0.04545455, 0.27272727, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.5       , 0.        , 0.        , 0.        ,\n",
       "        0.36      , 0.14      ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the P_k^i for each cluster in the partition i\n",
    "P_k_i = p_cont_i / p_k\n",
    "P_k_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2       , 0.16666667, 0.24666667, 0.08      , 0.14      ,\n",
       "       0.12      , 0.04666667])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a constant and it's the ratio of points in each cluster in pi_i\n",
    "P_i = p_cont_i.sum(axis=0)\n",
    "P_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53125   ],\n",
       "       [0.54132231],\n",
       "       [0.3992    ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the norm of each row of P_k^i\n",
    "norm_P_k_i = np.linalg.norm(P_k_i, axis=1, ord=2).reshape(-1, 1) ** 2\n",
    "norm_P_k_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17120000000000002"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_P_i = np.linalg.norm(P_i, ord=2) ** 2\n",
    "norm_P_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19833333],\n",
       "       [0.15878788],\n",
       "       [0.13306667]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_k * norm_P_k_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31898787878787876"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility = (p_k * norm_P_k_i).sum() - norm_P_i\n",
    "utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.cluster import adjusted_rand_score\n",
    "# adjusted_rand_score(y, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average the utility to get the final consensus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 7)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_partitions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2630467145245428"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_p(pi, pi_i):\n",
    "    '''\n",
    "    The information that we need to extract from the contingency table is:\n",
    "        - $p_{kj}^{(i)}$ = the ratio of objects in consensus cluster k that belong to cluster j based on partition i\n",
    "        - $p_k$ = the number of objects in consensus cluster k (p_k+)\n",
    "        - $P_k^{i}$ = $p_{kj}^{(i)}/p_k$\n",
    "        - $P^(i) = the vector with the ratios of objects in each cluster based on partition i\n",
    "    '''\n",
    "    # Calculate contingency matrix (n)\n",
    "    cont_i = crosstab(pi, pi_i).count\n",
    "\n",
    "    # Get p-contingency matrix\n",
    "    p_cont_i = cont_i / cont_i.sum()\n",
    "\n",
    "    # Calculate the p_k for each cluster in PI\n",
    "    p_k = p_cont_i.sum(axis=1)\n",
    "    p_k = p_k.reshape(-1, 1)\n",
    "\n",
    "    # Get the P_k^(i)\n",
    "    P_k_i = p_cont_i / p_k\n",
    "\n",
    "    # This is a constant and it's the ratio of points in each cluster in pi_i\n",
    "    P_i = p_cont_i.sum(axis=0)\n",
    "\n",
    "    return p_k, P_k_i, P_i\n",
    "    \n",
    "def compute_U_c_i(p_k, P_k_i, P_i):\n",
    "    norm_P_k_i = np.linalg.norm(P_k_i, axis=1, ord=2).reshape(-1, 1)\n",
    "    norm_P_i = np.linalg.norm(P_i, ord=2)\n",
    "    utility = (p_k * norm_P_k_i).sum() - norm_P_i\n",
    "    return utility\n",
    "\n",
    "ls_utilities = []\n",
    "for pi_i in ls_partitions_labels:\n",
    "    p_k, P_k_i, P_i = extract_p(pi, pi_i)\n",
    "    utility = compute_U_c_i(p_k, P_k_i, P_i)\n",
    "    ls_utilities.append(utility)\n",
    "\n",
    "np.mean(ls_utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5923326221845838"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "adjusted_rand_score(pi, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
